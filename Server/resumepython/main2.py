import pandas as pd
import numpy as np
import PyPDF2
import spacy
from spacy.matcher import Matcher
import re
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.models import Sequential
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import regularizers
from gensim.models import KeyedVectors

# Load the pre-trained spacy model
nlp = spacy.load('en_core_web_sm')

# Extract skills from the parsed text
def extract_skills(doc):
    skills = []
    for entity in doc.ents:
        if entity.label_ in ["SKILL", "EDUCATION", "EXPERIENCE", "AWARD", "LANGUAGE", "PROJECT", "ORG", "GPE","ACHIEVEMENTS"]:
            skills.append(entity.text.strip())
    return skills

# Extract text from PDF file
def extract_pdf_text(path):
    with open(path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        doc = nlp(text)
        # Extract skills from the parsed text
        skills = extract_skills(doc)
        text=",".join(skills)
    return text

def train_model(dataset_path):
    # Load the dataset
    dataset = pd.read_csv(dataset_path)

    # Remove rows with empty values
    dataset.dropna(axis=0, how='any', inplace=True)

    # Pre-process the text data
    requirements = dataset['Requirements'].tolist()
    requirements = [re.sub(r'[^\w\s]', '', str(req)) for req in requirements]

    # Tokenize the text data
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(requirements)
    word_index = tokenizer.word_index
    vocab_size = len(word_index) + 1
    sequences = tokenizer.texts_to_sequences(requirements)
    max_length = max([len(seq) for seq in sequences])
    text_pad = pad_sequences(sequences, maxlen=max_length, padding='post')

    # Prepare the labels
    requirement_labels = []
    for label in dataset['Match']:
        if label == 'Yes':
            requirement_labels.append(1)
        else:
            requirement_labels.append(0)

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(text_pad, np.array(requirement_labels), test_size=0.5, random_state=42)

    # Load the pre-trained word embeddings
    word_vectors = KeyedVectors.load_word2vec_format(r"g:/resumepython/GoogleNews-vectors-negative300.bin", binary=True, encoding='utf-8')

    # Create the embedding matrix
    embedding_dim = 300
    embedding_matrix = np.zeros((vocab_size, embedding_dim))
    for word, i in word_index.items():
        if word in word_vectors.key_to_index:
            embedding_matrix[i] = word_vectors.word_vec(word)

    # Create the model
    model = Sequential()
    model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))
    model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))
    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))
    
    adam = Adam(learning_rate=0.001)
    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])

    # Define callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=5)
    checkpoint = ModelCheckpoint("best_model.hdf5", monitor='val_accuracy', save_best_only=True, mode='max')

    # Train the model
    model.fit(X_train, y_train, validation_split=0.1, epochs=50, callbacks=[early_stopping, checkpoint])

    # Evaluate the model on the testing set
    _, accuracy = model.evaluate(X_test, y_test)
    print("Accuracy: %.2f%%" % (accuracy * 100))

    return model, tokenizer, max_length

def test_model(model, tokenizer, max_length, test_data_path, requirements):
    test_data = pd.read_csv(test_data_path)
    
    for index, row in test_data.iterrows():
        # Extract text from the resume
        text = extract_pdf_text(row['Resume'])
        print(f"\nTesting resume {index+1}:")
        print(text)
        
        # Pre-process the text data
        text = re.sub(r'[^\w\s]', '', text)
        text_sequence = tokenizer.texts_to_sequences([text])
        text_pad = pad_sequences(text_sequence, maxlen=max_length, padding='post')

        # Predict the match for the resume
        prediction = model.predict(text_pad)[0][0]
        if prediction > 0.5:
            match = "Yes"
        else:
            match = "No"

        # Pre-process the requirements
        requirements = [re.sub(r'[^\w\s]', '', str(req)) for req in requirements]
        sequences = tokenizer.texts_to_sequences(requirements)
        requirements_pad = pad_sequences(sequences, maxlen=max_length, padding='post')

        # Predict the match for each requirement and check if all match
        for i in range(requirements_pad.shape[0]):
            requirement_prediction = model.predict(requirements_pad[i:i+1])[0][0]
            if requirement_prediction < 0.5:
                print("Requirement", i+1, "does not match")
                match = "No"
                break

        if match == "Yes":
            print("The resume matches all the requirements")
        else:
            print("The resume does not match all the requirements")



#model, tokenizer, max_length = train_model(r"g:\resumepython\my_updated_dataset.csv")
# model, tokenizer, max_length = train_model(r"g:\resumepython\parsed_resume_dataset.csv")
model, tokenizer, max_length = train_model(r"g:\resumepython\train_data.csv")
resume_path = r"g:\resumepython\my_updated_dataset.csv"
requirements = ["""P,Healthcare Business Analyst,Healthcare Business Analyst,FL
Lessard,Junior Healthcare Business Analyst,TX
Bolt,TX,Assisted,Business Analysis,Data Science,Healthcare Industry,eCornell,Coursera,SQL Power BI,EDUCATION,M.S. Business Analytics
University of Houston,Dallas,TX,B.S. Business
Administration
University at Buffalo
Buï¬€alo,NY,Data Visualization
Able,eHealthcare Business Analyst,linkedin.com/in/carolinemadden,Miami,MADDEN"""]
test_model(model, tokenizer, max_length, resume_path, requirements)